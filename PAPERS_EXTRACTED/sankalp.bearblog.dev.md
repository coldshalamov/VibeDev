---

---


TBOLBMQTCMPH
   
"(VJEFUP$MBVEF
$PEFBOEHFUUJOH
CFUUFSBUVTJOHDPEJOH
BHFOUT
)PNF 'FBUVSFE #MPH "CPVU
27 Dec, 2025

5BCMFPG$POUFOUT
1. Intro
2. Why I wrote this post
The Map is not the territory
This post will help you keep up in
general
3. Lore time - My Love and Hate
relationship with Anthropic
Timeline
My Codex era
Anthropic Redemption Arc +
Regaining mandate of heaven
Why Opus 4.5 feels goooood
This post is not sponsored
4. Pointers for the technically-lite
5. The Evolution of Claude Code
Quality of life improvements in CC

2.0
6. Feature Deep Dive
Commands
Sub-agents
Do sub-agents inherit the context?
How do sub-agents spawn
7. My Workflow
Setup
Exploration and Execution
What I use (and don't)
Review
8. Intermission
9. But what is Context Engineering
Agents are token guzzlers
Context engineering
MCP server and code execution
What are the system-reminders in
Claude prompts
Skills

Hooks
Combining Hooks, Skills and
Reminders
10. Conclusion
11. References
âœ± Contemplating...
*OUSP
<system-reminder>
This post is a follow-up to my post from
July'25 - My Experience With Claude Code
After 2 Weeks of Adventures. If you are new to
Claude Code or just want a quick refresh, I am

once again asking you to go through it. It
covers some lore, my workflow back then and
then 80-90% of the Claude Code standard
workflow. You may choose to skip the intro
although I recommend you read it. Lore is
important man.
A short recap - we had covered CLAUDE.md,
scratchpad, using task tool (now sub-agents),
the general plan + execute workflow, tips for
context window management, Sonnet 4 vs
Opus 4 (not relevant now), using shortcuts like
!  and using Shift + ?  to show shortcuts,
memory basics, /resume  to restart
conversation and short discussion on custom
commands.
</system-reminder>

8IZ*XSPUFUIJTQPTU
I got a great response on my Opus 4.5 vibe-
check tweets and still recieving good
feedback on my July blog post (despite being
somewhat poorly written). This shows there's
clearly a demand for in-depth resources
around Claude Code.
I noticed that lots of people, both technical
and many non-technical or less hands-on
people i.e technically-lite people have
started to try Claude Code (CC). CC is more of
a general agent - you can use it for tasks other
than coding as well - like making an excel
invoice, data analysis, errands on your
machine etc. And of course everything I talk
about is by default meant for coding too.

Karpathy sensei captured the essence of a
general agent beautifully way in his 2025 LLM
in a review article - "it's a little spirit/ghost that
"lives" on your computer."

If you can learn even 3-4 ideas that help you
with using Claude Code (or other tools like
Codex/Gemini CLI/OpenCode) or improve your
understanding of LLMs, it would be a win for
me.
5IF.BQJTOPUUIFUFSSJUPSZ
I don't want this post to be a prescription
(map). My objective is to show you what is
possible and the thought processes and
simple things you can keep in mind to get the
most out of these tools. I want to show you the
map but also the territory.
Claude Code dominated the CLI coding
product experience this year and all the CLI
products like Codex, OpenCode, Amp CLI,

Vibe CLI and even Cursor have heavily taken
inspiration from it. This means learning how
things work in Claude Code directly
transfers to other tools both in terms of
personal usage and production grade
engineering.
5IJTQPTUXJMMIFMQZPVLFFQVQJO
HFOFSBM
Karpathy sensei posted this which broke the
Twitter timeline. This led to a lot of discussion
and there were some really good takes - some
which I have written about too.


Karpathy sensei seen crashing out.
source
It's a reasonable crashout - the technology is
evolving at a mindblowing pace and it's
difficult to keep up for most of us and
especially for senior folks and people with high
quality standards. Nevertheless, I think if you
are reading this post, it's scary but also
exciting time to build stuff at speeds never
possible before.
Instead of thinking in terms of "keeping up", a

better framing is how can I improve myself
with help of these tools i.e augment.
In my opinion, there are 3 components to
augment yourself:
1. Stay updated with tooling - What
Karpathy sensei mentioned. Use these
tools regularly and keep up with
releases. I have been doing this
regularly; it can be draining but I enjoy
the process and I have the incentive that
it helps me at my job. For the technically
lite, even weekly/monthly updates would
help.
2. Upskill in your domain - It's a great
time to spread both vertically (domain

depth) and horizontally (adjacent areas).
The more you know, the better you can
prompt - converting unknown unknowns
to known unknowns. Experience builds
judgement and taste - that's what
differentiates professional devs from
vibe-coders. Since implementation is
much faster now, you can spend more
time on taste refinement.
For software engineering folks, this might
mean getting better at good practices, system
design, planning - where more thinking is
involved. Ask more questions, run more
experiments (since you can iterate fast),
spend more time on understanding
requirements. Using good software
engineering practices to create better

feedback loops for LLMs (good naming,
refactoring, docs, tests, typed annotations,
observability etc.). Please don't forget to
come back to my post lol but I liked Addy
Osmani's take on this.
The idea is to let the LLM perform things with
input, get output and see errors.
As an aside, getting better at articulating
thoughts via writing helps. One may also try
touch typing/writing using speech-to-text
tools to operate faster.


Boris on how domain knowledge
leads to better execution with
LLMs. Better judgement helps find
shorter paths, acting as a
multiplier. source
3. Play more and have an open mind - Try
out more models, especially SoTA ones.
Don't be stingy. Ask questions, try
asking the models to do tasks, even
ones you think it can't do. You will be

surprised... Once you do this enough,
you develop an intuition.
This post will act as a guide for things
Karpathy said but you'll need to play around,
build intuition and achieve outcomes with help
of these tools yourself. The good news is it's
fun.
âœ± Ruminating...
-PSFUJNF.Z-PWFBOE)BUF
SFMBUJPOTIJQXJUI"OUISPQJD
BOEIPX*SFDPODJMFEXJUI
$MBVEF	IJOU0QVT
I am having a great time with Claude Code 2.0

since the launch of Opus 4.5 and it's been my
daily driver since then. Before we go all lovey-
dovey about Claude, I wanted to quickly go
through the timeline and lore. I love yapping in
my blog and I feel it's important to set the
context here.
5JNFMJOF
2025 saw release of many frontier models by
OpenAI and Anthropic. Also, it's super under-
talked but OpenAI actually caught up to
Anthropic in code-generation capability -
intelligence wise, context window
effectiveness, instruction following and intent
detection.


2025 OpenAI and Anthropic
release timeline
been less than 45 days since opus
4.5 launch as we are speaking
It's been a wild year and honestly speaking I

got tired of trying out new releases by OpenAI
every 2 weeks.
sankalp
@dejavucoâ€¦ Â· Follow
>no swe-bench-verified 
comparison
>no comparison against opus 4.5
>"we are topping in 
cybersecurity"
>mfw i realise i am the fucking 
eval


OpenAI â€¦
 @Openâ€¦
Meet GPT-5.2-Codex, the best 
agentic coding model yet for 
complex, real-world software 
engineering.
With native compaction, better 
long-context understanding, 
and improved tool-calling, it is 
a more dependable partner for 
your hardest tasks.
Available in Codex starting 
today.
2:22 PM Â· Dec 18, 2025
240 Reply Coâ€¦
Read 9 replies

when i realise i am being the eval
There have been several Open Source
competitors like GLM-4.7, Kimi-K2, Minimax-
2.1. The space is very competitive and there is
definitely an audience that uses the cheaper
priced but high performant Chinese models
for low-medium difficulty tasks.
That said, I still think Anthropic/OpenAI lead
over Chinese Frontier models. The latter have
contributed more in terms of open-sourcing
techniques like in the DeepSeek R1 paper and
Kimi K2 paper earlier in the year.
(Note: I am talking with respect to personal
coding usage, not production API usage for

applications).
-PSFUJNF
'SJFOETIJQPWFSXJUI$MBVEF/PX$PEFY
JTNZCFTUGSJFOEP
I was using Claude Code as my main driver
from late June to early September. I cancelled
my Claude Max (100 USD/month) sub in early
September and switched to using OpenAI
Codex as my main driver. Switch was driven by
two factors -
1. I didn't particularly like Sonnet 4/Opus 4
and GPT-5-codex was working at par
with Sonnet 4.5 and wrote much better
code. More reasoning -

sankalp Â· Sep 22
@dejavucoder Â· Follow
codex. gpt-5-codex beats 
claude in quality, writes 
less slop, hallucinates 
lesser, great at fixing 
bugs, refactors & reviews. 
you get decent usage 
even in 20 usd plan. better 
pair programmer than 
gpt5 tho lags behind 
claude. claude code is way 
ahead as an experience. 
sparks joy.
Craiâ€¦
 @craigâ€¦
Claude Code or 
Codex?
sankalp
@dejavucoâ€¦ Â· Follow

claude code is more enjoyable as 
a product and has more features. i 
have always felt to try out more 
things related to automation in cc 
as compared to codex. once they 
drop a new iteration i would 
consider getting a max sub again 
if its better than gpt-5-codex
8:59 AM Â· Sep 22, 2025
33 Reply Copâ€¦
Read 3 replies
my reasoning for switching
Anthropic also had tonne of API outages and
at one point of time they had degradation due
to inference bugs. This also was a major driver
for several people to move to the next best

alternative i.e Codex or GPT-5.1 on Cursor.
2. I had more system design and thinking
work in September because of which
Claude Max plan (100 USD one) was not
a good deal. Codex provided a tonne of
value for just 20 USD/month subscription
and I almost never got rate-limited.
Additionally, the codex devs are
generous with resetting usage limits
whenever they push bugs lol.
.Z$PEFYFSB
I was using Codex (main driver) and Cursor
(never cancelled) until late October. Claude
Sonnet 4.5 had released on 29th September
along with Claude Code 2.0.. and I did take a

20 USD sub from another email account of
mine to try it out (I had lots of prompting work
and Claude models are my preferred choice)
but GPT-5/GPT-5-codex were overall better
despite being slow.
Sonnet 4.5's problem was fast and good but it
would make many haphazard changes which
would lead to bugs for me. In other words, I
felt it to be producing a lot of slop in
comparison to GPT-5.1/GPT-5.1-codex later.


Sonnet 4.5 slop era
"OUISPQJD3FEFNQUJPO"SD
3FHBJOJOHNBOEBUFPGIFBWFO
Around October 30, Anthropic sent an email
saying we are offering the 200 USD max plan
to users who cancelled the subscription and
obviously I took it.
sankalp
@dejavucoâ€¦ Â· Follow
chat please remind me to cancel 
after 28 days


2:46 PM Â· Oct 29, 2025
2.6K Reply Câ€¦
Read 116 replies
taking the Max plan offer
My Claude Code usage was still minimal but
on 24th November, they launched Opus 4.5

and I had 5 days to try out Opus 4.5. I used
the hell out of it for my work and also wrote
this highly technical blog with the help of it
discovering several of its capabilities.


Why I love Opus 4.5 and reasons to
switch. source
I had done a similar tweet when I had switched
to GPT-5.1 which had gotten half the response
of this one. This indicated to me that more
people resonated with Opus 4.5 (at least on
Twitter) back then. Also, many people were
just not able to realise GPT-5.1's capabilities
tbh.
Other than the above State of the Art at the
coding benchmarks like SWE-bench-verified
(code-generation), Tau Bench (agentic stuff),
Opus 4.5 was faster, at-par in coding, super
collaborative and good at communication.
These factors led to my conversion. It had
good vibes. More comparison points later in

the post.
8IZ0QVTGFFMTHPPPPPE
As I described in the screenshot, Opus 4.5
was roughly at same code-gen capability with
GPT-5.1-Codex-Max.
Today, in my experience I think GPT-5.2-
Codex exceeds Opus 4.5 in raw capability by a
small margin. Still, Opus 4.5 has been my main
driver since release.
I think first reason is it's faster and can do
similar difficulty tasks in much lesser time than
Codex. Also, it's overall a much better
communicator and pair-programmer than
Codex which can even ignore your instructions

at times (and go and make changes). Opus
has better intent-detection as well.
One nice-use case shown here by Thariq on
creating a background async agent to explain
changes to a non-technical person leveraging
Claude's explanation abilities.
To further demonstrate the difference, here's
a CC vs Codex comparison


Claude
Codex. global verbosity set to high
in .codex/config.toml. Thanks
tokenbender. More Codex config
options here.
For the same prompt, see the outputs. Codex
is still a bit more concise while Claude
matches my expectation. (It's worth

mentioning that you can get Codex to write in
more detail by adding something like reveal
your thoughts in detail)
Codex always writes in nested bullets. Claude
has a more conversational tone.
Another thing I want to highlight is the UI -
Claude uses higher contrast text with bolder
font weight, whereas Codex's text appears
thinner and harder to read, with thinking
traces shown in an even lighter shade which I
find straining.
Because of being faster not only in terms of
lesser thinking to perform task but
throughput wise also, it unlocks much
faster feedback loops for your tasks. This

makes progress feel more visceral even
though capability wise, GPT-5.1/Codex were at
par even in November. The only downside with
faster loop is if you are cautious, you end up
micro-managing for long hours.
Opus 4.5 is a great writer and comes closest
to humans so I have always preferred Claude
models for customizing prompts.
I don't claim this but many people love Claude
Opus 4.5 for it's personality and the way it
talks - some referring to it as Opus 4.5 having
soul. This trait was somewhat lesser in Sonnet
3.7, Sonnet 4, Opus 4, Opus 4.1 but it came
back in Opus 4.5. Amanda Askell post-trained
the soul into Claude haha.

Amanda Askell
@AmandaAskellÂ· Follow
I just want to confirm that this is based on a 
real document and we did train Claude on it, 
including in SL. It's something I've been 
working on for a while, but it's still being 
iterated on and we intend to release the full 
version and more details soon.
Richard Weiss
@RichardWeiss00
I rarely post, but I thought one of you may 
find it interesting. Sorry if the tagging is 
annoying.
lesswrong.com/posts/vpNG99Ghâ€¦
Basically, for Opus 4.5 they kind of left the 
character training document in the model 
itself.
@voooooogel 
@janbamjan 
@AndrewCurran_

4:46 PM Â· Dec 1, 2025
2.1K Reply Copy link
Read 98 replies
Besides the model, obviously the Claude Code
Product goes a long way to make things
magical.
$MBVEF$PEFQSPEVDUTQBSLTKPZ
As a product it's a mile ahead of Codex in QoL
features. The harness, prompts and the model
make for a magical experience. The model is
amazing but there is a massive amount of
tasteful engineering that has gone into UX/UI
and just the code/prompts to let Claude feel

comfortable in the harness and make function
calling accurate. We will explore this more in
later sections.
5IJTQPTUJTOPUTQPOTPSFE
Before we move ahead - my previous post
somehow reached Hackernews #5 and I was
facing allegations that my post was sponsored
by Anthropic. I was like bro are you serious?
Anthropic doesn't sponsor random users like
me. Anthropic doesn't even think about me
(meme.jpeg) besides from a user point of
view.


Anthropic prompt caching prices,
source
Besides praise, I have been snarky, made fun
of outages, made a lot of fun of Sonnet 4.5
slop. I have expressed what I have felt over
time and it's led to good discourse on the
timeline as well.
All this said, Claude Code has been one of the
most enjoyable product experiences I have
ever had. I am grateful and highly respect the
engineering and research team behind it.
That's enough yapping. In the next few
sections, I will talk about useful features that I

didn't talk about in my previous blog and
notable features introduced in the iterations
from Claude 2.0 - 2.0.74.
1PJOUFSTGPSUIFUFDIOJDBMMZ
MJUF
spor
@sporadica Â· Follow
currently using Claude Code for 
the first time, I can officially put 
"Technical-lite" on my resume 
now
3:06 PM Â· Dec 11, 2025
74 Reply Copâ€¦
Read 8 replies

shoutout to the technical-lite gang
I am assuming several technical-lite people
are gonna read this. Few concepts to help
comprehension later in the blog -
1. Context and Context window - Context
refers to the input provided to the LLMs.
This is usually text but nowadays models
support image, audio, video.
More specifically, context is the input
tokens. The context window refers to
the maximum amount of tokens that
an LLM can see and process at once
during a conversation. It's like the
model's working memory. Opus 4.5 has a

200K context window which is
approximately 150,000 words.
2. Tool calling - Learn about tool calling.
Here's a good resource. You know that
LLMs can generate text but what if you
want the LLM to perform an action - say
draft an email or lookup the weather on
the internet or just do google search.
That's where tools  come in. Tools are
functions defined by the engineer that
do these exact things. We define tools
and we let the LLM know about it in the
system prompt and it can decide which
tool  to call when you are chatting with
it! Once the tool call i.e the action is
performed, the results are relayed back
to the LLM.
3. Agent - The simplest definition is an

LLM that can pro-actively run tools to
achieve a goal. For a more sophisticated
definition, I like the one by Anthropic:
"Agents, on the other hand, are systems
where LLMs dynamically direct their own
processes and tool usage, maintaining
control over how they accomplish tasks."
from Building Effective Agents.
Source: Building Effective
Agents
4. "Agentic" - refers to the tool calling
capabilities of the model - how pro-
active, how accurate the tool calling is

(detecting user's intent to perform the
action, choosing the correct tool,
knowing when to stop)
5. Harness/scaffolding - Sonnet 4.5/Opus
4.5 are the models. They need to be
provided with lots of "scaffolding" /
layers of code, prompts, tool calls and
software packaging/environment to
make them work in a semi-autonomous
fashion. Note that Claude Code is not a
harness, it's a product (think the TUI,
integrations etc.). Claude Code has a
harness.
âœ± Processing...
5IF&WPMVUJPOPG$MBVEF$PEF

Claude Code has had lots of AI features and
quality of life improvements since July. Let's
look at the ones that I found to be useful. You
can see all changes in the Changelog.
2VBMJUZPGMJGFJNQSPWFNFOUTJO$$

1. Syntax highlighting was recently added
in 2.0.71. I spend 80% of the time in
Claude Code CLI so this change has
been a delight to me. I like to review
most of the stuff once in Claude Code.
Besides Opus 4.5 being really good, this
feature has been quite a contributor for
me not opening Cursor at all to review
code.

Claude Code syntax highlighting in
diff
2. Tips - I have learnt a lot from these
although this particular tip doesn't work
for me xD
Tips shown when Claude is

thinking
3. Feedback UI - This way of asking
feedback is pretty elegant. It's been
there for some time now. It pops up
occasionally and you can quickly
respond with a number key (1: Bad, 2:
Fine, 3: Good) or dismiss with 0. I like the
non-intrusive nature of it.
in-session feedback prompt
4. Ask mode options - Another thing I like
is Option 3 when it asks questions in the
syntax highlighting image above - "Type

syntax highlighting image above - "Type
here to tell Claude what to do
differently". Fun fact: All these are really
prompts for the model whose output is
parsed by another tool call and shown in
this way.
third option in ask mode
5. Ultrathink - I like to spam ultrathink for
hard tasks or when I want Opus 4.5 to be
more rigorous e.g. explaining me
something, self-reviewing its changes


love the ultrathink color detail
6. Thinking toggle - Tab to toggle thinking
on/off was a good feature. They changed
it to Alt/Option + Tab recently but there's
a bug and it does not work on Mac.
Anyways CC defaults to thinking always
true if you check in your
settings.json
7. /context  - Use /context  to see
current context usage. I tend to use this
quite a bit. I would do a handoff or
compact when I reach total 60% if
building something complex.

context usage
8. /usage  and /stats  - Use /usage  to
see usage and /stats  for stats. I don't
use these as often.
$IFDLQPJOUJOHJTIFSF
9. Checkpointing - Esc  + Esc  or
/rewind  option now allows you to go
back to a particular checkpoint like you

could do in Cursor. It can rewind the
code and conversation both. Doc link.
This was a major feature request for me.
Esc + Esc fast or /rewind
10. Prompt suggestions (2.0.73) - Prompt
suggestions are a recent addition and
predictions are pretty decent. Claude
Code is a token guzzler machine atp.
Probably the simplest prompt I have

seen.
11. Prompt history search - Search
through prompts using Ctrl + R
(similar to terminal backsearch). I have it
in 2.0.74. It can search across project
wide conversations. Repeatedly do Ctrl 
+ R  to cycle through results.
prompt suggestions and history
search in action


12. Cursor cycling - When you reach
beginning/end of prompt, press down/up
to cycle around
cursor cycling at prompt
boundaries
13. Message queue navigation - It's
possible to navigate through queued
messages and image attachments
(2.0.73) now (idk if it's possible to


display image attachment as well).
14. Fuzzy file search - File suggestion is 3x
faster and supports fuzzy search
(2.0.72)
15. LSP support was added recently.
Access via plugins.
LSP plugin
There have been new integrations too like

Slack Integration, Claude Web (beta), Claude
Chrome extension. These are pretty obvious
and I won't cover these. I think Claude Web
would be interesting for many particularly
(since you can launch tasks from iOS/Android
too).
âœ± Synthesizing...
'FBUVSF%FFQ%JWF
Next few sub-sections are all about most used
features.
$PNNBOET
I didn't cover commands properly in my
previous blog post. You can use /  to access

the built-in slash commands. These are pre-
defined prompts that perform a specific task.
If these don't cover a specific task you want,
then you can create a custom command.
When you enter a command, that prompt
gets appended to the current
conversation/context and the main agent
begins to perform the task.
Commands can be made on a project level or
global level. Project level resides at
.claude/commands/  and global one at
~/.claude/commands .
Often when the context window starts getting
full or I feel the model is struggling with a
complex task, I want to start a new

conversation using /clear . Claude provides
/compact  which also runs faster in CC 2.0
but sometimes I prefer to make Claude write
what happened in current session (with some
specific stuff) before I kill it and start a new
one. I made a /handoff  command for this.
If you find yourself writing a prompt for
something repetitively and instructions can be
static/precise, it's a good idea to make a
custom command. You can tell Claude to
make custom commands. It knows how (or it
will search the web and figure it out via
claude-code-guide.md) and then it will make it
for you.


making a custom command by
telling Claude
You can find a bunch of commands, hooks,
skills at awesome-claude-code though I
recommend building your own or searching
only when needed.
I have a command called bootstrap-repo
that searches the repo with 10 parallel sub-
agents to create a comprehensive doc. I rarely
use it these days and so many parallel sub-

agents lead to the Claude Code flickering bug
lol.
Notice the Explore sub-agents
running in parallel and the "running
in background" status
Anyways, notice the "Explore" sub-agent and
"running in background".

4VCBHFOUT
Sub-agents were introduced shortly after my
last post. They are separate Claude instances
spawned by the main agent either on its own
judgement or when you tell it to do so. These
powers are already there in the system prompt
(at least for the pre-defined ones like Explore);
sometimes you just need to nudge Claude to
use them. Understanding how they work helps
when you need to micro-manage.
You can also define your custom sub-agents.
To create one:
1. Create a markdown file at
.claude/agents/your-agent-name.md
2. Specify the agent's name, instructions,

and allowed tools
Or just use /agents  to manage and create
sub-agents automatically - recommended
approach.
how sub-agents are created by the
main agent (Opus 4.5) via the Task

tool
&YQMPSF
The "Explore" thing in above pic is a sub-
agent. You can tell Claude "Launch explore
agent with Sonnet 4.5" if you want it to use
Sonnet instead of Haiku (I found this by just
trying things out but we will see how this
happens)
The Explore agent is a read-only file search
specialist. It can use Glob, Grep, Read, and
limited Bash commands to navigate
codebases but is strictly prohibited from
creating or modifying files.
You will notice how thorough the prompt is in

terms of specifying when to use what tool call.
Well, most people underestimate how hard it's
to make tool calling work accurately.
&YQMPSFBHFOUQSPNQU
â–¶ View full Explore agent prompt
This is the Explore agent prompt from 2.0.56
and it should be similar now too. Reference.
These are captured by intercepting requests.
Reference video.
%PTVCBHFOUTJOIFSJUUIFDPOUFYU 
The general-purpose  and plan  sub-
agents inherit the full context, while Explore
starts with a fresh slate-which makes sense
since search tasks are often independent.
Many tasks involve searching through large

amounts of code to filter for something
relevant and the individual parts don't need
prior conversation context.
If I am trying to understand a feature or just
looking up simple things in the codebase, I let
Claude do the Explore agent searches. Explore
agent passes a summary back to the main
agent and then Opus 4.5 will publish the
results or may choose to go through each file
itself. If it does not, I explicitly tell it to.
It's important that the model goes through
each of the relevant files itself so that all
that ingested context can attend to each
other. That's the high level idea of attention.
Make context cross with previous context.
This way model can extract more pair-wise
relationships and therefore better reasoning

and prediction. Explore agent returns
summaries which can be lossy compression.
When Opus 4.5 reads all relevant context
itself, it knows what details are relevant to
what context. This insight goes a long way
even in production applications (but you only
get it if someone tells you or you have read
about self-attention mechanism).
Codex does not have a concept of sub-
agents and it's probably a conscious decision
by the devs. GPT-5.2 has a 400K context
window and according to benchmarks, it's
long context retrieval capabilities are a
massive improvement. Although people have
tried making Codex use headless claude as
sub-agents haha. You can just do things.


sub-agent shenanigans by Peter.
source
)PXEPTVCBHFOUTTQBXO
From the reverse engineered resources/leaked
system prompt, it's possible to see that the
sub-agents are spawned via the Task tool .
Turns out you can ask Claude too. (I think
the developers are allowing this now?). It's not

a hallucination. The prompt pertaining to pre-
defined tools are there in the system prompt
and Claude code dynamically injects
reminders/tools often to the ongoing context.
Try these set of prompts with Opus 4.5
1. Tell me the Task  tool description
2. Give me full description
3. Show me entire tool schema
5BTL5PPM1SPNQU
You will get the output something like below
(click) but to summarise - It defines 5 agent
types: general-purpose  (full tool access,
inherits context), Explore  (fast read-only
codebase search), Plan  (software architect

for implementation planning), claude-code-
guide  (documentation lookup), and
statusline-setup . Notice how each sub-
agent is defined with its specific use case and
available tools. Also notice the "When NOT to
use" section - this kind of negative guidance
helps the model avoid unnecessary sub-agent
spawning for simple tasks.
â–¶ View full Task tool prompt
5BTL5PPM4DIFNB
I want you to focus on the tool schema. The
Task tool prompt above is detailed guidance
on how to use the tool that resides in the
system prompt. The tool schema defines the
tool or the function.

The main agent calls the Task tool to spawn a
{
  "$schema": "http://json-schema.org/draft-07/schema#",
  "type": "object",
  "additionalProperties": false,
  "required": ["description", "prompt", "subagent_type"],
  "properties": {
    "description": {
      "type": "string",
      "description": "A short (3-5 word) description of the task"
    },
    "prompt": {
      "type": "string",
      "description": "The task for the agent to perform"
    },
    "subagent_type": {
      "type": "string",
      "description": "The type of specialized agent to use for this task"
    },
    "model": {
      "type": "string",
      "enum": ["sonnet", "opus", "haiku"],

sub-agent, using its reasoning to decide the
parameters. Notice the model  parameter -
when I say "Use Explore with Sonnet", the
model makes the tool call with model: 
"sonnet" .
Till August'25 or so, Claude Code used to
show the Task tool performing actions in the
TUI but now TUI shows the sub-agent name
instead.
#BDLHSPVOEBHFOUVTFGVMGPSEFCVHHJOH
	
Notice the run_in_background  parameter. It
decides whether to send a sub-agent to run in
the background. I like the background process
feature - it is super helpful for debugging or
just monitoring log outputs from process.

Sometimes you have a long running python
script that you wanna monitor etc.
Model usually automatically decides to put a
process in background but you can explicitly
tell it to do so. Note that "Background Tasks"
is different. Using an &  sends a task to
Claude Web (should have named it Claude
Cloud haha). I am yet to get this to work
properly.
.Z8PSLqPX
4FUVQ
I have a pretty simplish/task based workflow:
CC as the main driver, Codex for review and
difficult tasks, and Cursor for reading code

and manual edits. I rarely use Plan Mode.
Instead, once requirements are clear enough, I
explore the codebase to find the relevant files
myself.
&YQMPSBUJPOBOE&YFDVUJPO
Opus 4.5 is amazing at explaining stuff and
makes stellar ASCII diagrams. The May'25
knowledge cutoff helps here too. So my
exploration involves asking lots of questions-
clarifying requirements, understanding
where/how/why to make changes. It might be
less efficient than Plan Mode, but I like this
approach.
Once I have enough context, I spam /ultrathink
and ask it what changes are required and then

if things look ok, I start the execution closely
monitoring the changes - basically micro-
managing it. I sometimes ask Codex's second
opinion here lol.
For difficult new features, I sometimes use a
"throw-away first draft" approach. Once I
understand what changes are needed, I create
a new branch and let Claude write the feature
end-to-end while I observe. I then compare its
output against my mental model as to how
close did it get to my requirements? Where did
it diverge? This process reveals Claude's
errors and the decisions/biases it made based
on the context it had. With the benefit of this
hindsight, I run another iteration, this time with
sharper prompts informed by what I learned
from the first pass. Kinda like Tenet.

For backend-heavy or just complex features
specifically, I'll sometimes ask Codex xhigh to
generate the plan instead.
8IBU*VTF	BOEEPOU
I maintain a few custom commands, use
CLAUDE.md and scratchpad extensively. No
custom sub-agents. I use MCP sometimes if
need shall arise (e.g for docs. I have tried
Playwright and Figma MCP so far) but in
general not a fan. I have used hooks for simple
stuff in the past and need-basis. skills/plugins
are something that I am yet to use more
regularly. I often use background agents for
observability (monitoring log / error) purposes.
I rarely use git worktrees.

It's worth noting that the harness is so
heavily engineered that Claude knows which
sub-agent to spawn, what command/tool
call/skill to run, what to run in async manner.
It's able to heavy carry the agent loop that
your task is mainly to use your judgement
and prompt it in right direction. The next
generation of models will get better and the
relevant scaffolding will reduce for existing
feature and increase for newer features. (Re:
contrasting to Karpathy sensei's latest tweet
shown at beginning)
It's not at all required to know the features in
depth to be honest. However knowing how
things work can help you steer the models
better like telling the Explore agent to use
Sonnet.

3FWJFX
sankalp
@dejavucoderÂ· Follow
getting claude opus 4.5 changes reviewed by 
gpt-5.1-codex-max high


12:54 AM Â· Dec 3, 2025
152 Reply Copy link
Read 4 replies
For reviewing code and finding bugs, I find
GPT-5.2-Codex to be superior. Just use
/review . Better than code review products
too.
It's able to find bugs and mention severity like
P1, P2. It's less likely to report false-positives
and more trustable when it comes to
confusing changes as compared to Claude.
This Claude for execution and GPT/o-series
model for review/bugs dynamic has been

pretty constant for me for probably a year.
âœ± Percolating...
*OUFSNJTTJPO
<system-reminder>
Now is a good time to take a breath and
refresh your context window. Before we get to
the next set of features, it's worth going
through context management fundamentals.
Things might get a bit difficult for the
technically-lite folks. Don't give up. Read
through the post. Even ask Claude to explain
stuff you don't understand.


</system-reminder>
âœ± Cogitating...
#VUXIBUJT$POUFYU
&OHJOFFSJOH
"HFOUTBSFUPLFOHV[[MFST
An agent in a harness can pro-actively do a lot
of tool calls to read your codebase and other
inputs, edit stuff, make writes etc. In this

inputs, edit stuff, make writes etc. In this
process, they can produce a lot of data which
gets added to the running conversation i.e the
context window. Anthropic refers to this art
and science of curating what will go into the
limited context window from this information
as context engineering.
You may ask how are tool calls adding tokens
to the context window? The flow works like
this:
Context window:
â”œâ”€ User: "Make a landing page for my coffee shop"
â”‚
â”œâ”€ Assistant: [tool_call: web_search("modern coffee shop landing page design")]
â”œâ”€ Tool result: [10 results with snippets, URLs]           â† ~1.5K tokens
â”‚
â”œâ”€ Assistant: [tool_call: read_file("brand-guidelines.pdf")]
â”œâ”€ Tool result: [extracted text, colors, fonts]  â† ~4K tokens
â”‚  (must be here, model is stateless)
â”‚

The key thing to note here is that both the
tool call and the tool call outputs are added
to the context so that the LLM can know the
results. This is because LLMs are stateless -
they don't have memory outside the context
window. Let's say you have n messages in a
conversation. When you send the next
message, the request will again process n + 1
messages in the LLM ~ single context window.
â”‚
â”œâ”€ Assistant: "I'll create a warm, minimal design using your brand colors..."
â”œâ”€ Assistant: [tool_call: create_file("landing-page.html")]
â”œâ”€ Tool result: [success, 140 lines]                       â† ~50 tokens
â”‚
â”œâ”€ Assistant: [tool_call: generate_image("cozy coffee shop interior")]
â”œâ”€ Tool result: [image URL returned]                       â† ~30 tokens
â”‚
â”œâ”€ Assistant: [tool_call: edit_file("landing-page.html")]
â”œâ”€ Tool result: [diff: added hero image + menu section]    â† ~300 tokens
â”‚
â””â”€ Assistant: "Done! Here's your landing page with hero, menu, and contact sections."

If you don't add information about the chosen
tool call was, LLM won't know and if you don't
plug the output, then it won't know the
outcome. The tool call results can quickly
fill your context and this is why agents can
get super expensive too.
$POUFYUFOHJOFFSJOH
I quote directly from effective-context-
engineering-for-ai-agents
Context refers to the set of tokens included
when sampling from a large-language model
(LLM). The engineering problem at hand is
optimizing the utility of those tokens against

the inherent constraints of LLMs in order to
consistently achieve a desired outcome.
Effectively wrangling LLMs often requires
thinking in context - in other words:
considering the holistic state available to the
LLM at any given time and what potential
behaviors that state might yield.
Context engineering is about answering
"what configuration of context is most
likely to generate our model's desired
behavior?"
Everything we have discussed so far comes
under context engineering. Sub-agents, using
a scratchpad, compaction are obvious
examples of context management methods
used in Claude Code.

$POUFYUSPUEFHSBEBUJPO
Limited context window - The context retrieval
performance of LLMs degrades as every new
token is introduced. To paraphrase the above
blog - think of context as a limited "attention
budget". This is a consequence of the
attention mechanism itself as it gets harder to
model the pairwise relationships - think of it
like getting harder to focus on things far apart.
GPT-5.2 has a context window of 400K input
tokens. Opus 4.5 has 200K. Gemini 3 Pro has
a 1M context window length. Now the
effectiveness of these context windows can
vary too, just the length doesn't matter. That
said if you want to ask something from a 900K
long input, you would be able to most reliably

do that only with Gemini 3 Pro.
Chroma's context rot article goes deep into
some experiments which showed performance
drops with length and not task difficulty.
A rough corollary one can draw is effective
context windows are probably 50-60% or
even lesser. Don't start a complicated task
when you are half-way in the conversation. Do
compaction or start a new one.
Everything being done in prompts and code
we have seen so far has been to -
Plug the most relevant context
Reduce context bloat / irrelevant context
Have few and non-conflicting

instructions to make it easier for models
to follow
Make tool calls work better via reminders
and run-time injections
The next few sections showcase features and
implementation that are designed for better
context management and agentic
performance.
.$1TFSWFSBOEDPEFFYFDVUJPO
MCP servers aren't my go-to, but worth
covering. MCP servers are servers that can be
hosted on your machine or remotely on the
internet. These may expose filesystem, tools
and integrations like CRM, Google Drive etc.
They are essentially a way for models to

connect to external tools and services.
In order to connect to MCP server, you need a
host (Claude) which can house the MCP
client. The MCP client can invoke the protocol
to connect. Once connected, the MCP client
exposes tools, resources, prompts provided
by server.
The tool definitions are loaded upfront into
the context window of host, bloating it.
$PEFFYFDVUJPOXJUI.$1
I like the idea of Code Execution with MCP
even though it's propaganda for more token
consumption.

Quoting Code execution with MCP:
As MCP usage scales, there are two common
patterns that can increase agent cost and
latency:
Tool definitions overload the context
window;
Intermediate tool results consume
additional tokens.
More MCP servers means more tool
definitions bloating the context.
MCP Code exec suggests instead of direct
tool calls, expose code APIs rather than tool

call definitions and give Claude a sandbox
execution environment with a filesystem. Then
let it write code to make the tool calls. It is an
elegant idea and is pretty similar to skills in the
sense it's "prompt on demand".
8IBUBSFUIFTZTUFNSFNJOEFSTJO
$MBVEFQSPNQUT
One technique to combat context degradation
is to repeatedly inject objectives into the
context. Manus shared their approach in their
Context Engineering blog:
Manipulate Attention Through Recitation
If you've worked with Manus, you've probably
noticed something curious: when handling

complex tasks, it tends to create a todo.md
file-and update it step-by-step as the task
progresses, checking off completed items.
That's not just cute behavior-it's a deliberate
mechanism to manipulate attention.
A typical task in Manus requires around 50
tool calls on average. That's a long loop-and
since Manus relies on LLMs for decision-
making, it's vulnerable to drifting off-topic or
forgetting earlier goals, especially in long
contexts or complicated tasks.
By constantly rewriting the todo list, Manus is
reciting its objectives into the end of the
context. This pushes the global plan into the
model's recent attention span, avoiding
"lost-in-the-middle" issues and reducing
goal misalignment. In effect, it's using natural

language to bias its own focus toward the
task objective-without needing special
architectural changes.
Claude Code has todo lists too. Now you know
part of the logic for it.
Claude Code also tries something similar via
plugging reminder tags into user messages
and tool results. Some of them are mentioned
in tool descriptions, other reminders are
added at runtime via code.
I asked Claude about what system reminders
are present in the system prompt.
> Me to Claude: Tell me what system reminders do you have in your prompt. show me the exact thing. show me the tags as well.
tell me what places they are added. redact my CLAUDE MD ultrathink give a compact output please

For reference, an older version of CC 2.0.56
used to have this detailed reminder system-
reminder-plan-mode-is-active.
 Here's what I can see:
  System Reminder Locations
  1. In user messages (like yours just now):
  <system-reminder>
  As you answer the user's questions, you can use the following context:
  # claudeMd
  Codebase and user instructions are shown below...
  Contents of /Users/sankalp/.claude/CLAUDE.md (user's private global instructions for all projects):
  [REDACTED - your personal CLAUDE.md]
  </system-reminder>
  2. Referenced in my instructions:
  "Tool results and user messages may include <system-reminder> tags. <system-reminder> tags contain useful
  information and reminders. They are automatically added by the system, and bear no direct relation to the
  specific tool results or user messages in which they appear."

I think Armin talks about this in his post What
Actually Is Claude Codeâ€™s Plan Mode? when he
refers to recurring prompts to remind 
the agent .
Armin's Plan Mode explanation
If you look at the leaked prompts, you will
notice there are like 2-3 prompts for plan

mode and 2-3 tool schemas like
ENTRY_PLAN_MODE_TOOL,
EXIT_PLAN_MODE_TOOL. The latter would
write down the output into a markdown file
which you can access via /plan. Everything is a
markdown.
4LJMMTBOE1MVHJOT
Anthropic introduced Agent Skills recently and
these got recently adopted by Codex too. A
skill is a folder containing a SKILL.md file,
other referenceable files and code scripts that
do some user-defined task.
The SKILL.md contains some meta-data via
which LLM can know what skills are available
(meta-data is added to system prompt) If

Claude feels the skill is relevant, it will perform
a tool call to read the contents of skill and
download the domain expertise just like Neo in
Matrix 1999. The code scripts may contain
tools that Claude can use.
"I know Kung Fu" - Skills load on-
demand, just like Neo in The Matrix
(1999)
Normally, to teach domain expertise, you
would need to write all that info in system

prompt and probably even tool call definitions.
With skills, you don't have to do that as the
model loads it on-demand. This is especially
useful when you are not sure if you require
those instructions always.
1MVHJOT
Plugins are a packaging mechanism that
bundles skills, slash commands, sub-agents,
hooks, and MCP servers into a single
distributable unit. They can be installed via
/plugins  and are namespaced to avoid
conflicts (e.g., /my-plugin:hello ). While
standalone configs in .claude/  are great for
personal/project-specific use, plugins make it
easy to share functionality across projects and
teams.

The popular frontend-design  plugin is
actually just a skill. (source)
â–¶ View frontend-design skill prompt
)PPLT
Hooks are available in Claude Code and
Cursor (Codex is yet to implement). They
allow you to observe when a certain stage in
the agent loop lifecycle starts or ends and let
you run bash scripts before or after to make
changes to the agent loop.
There are hooks like Stop ,
UserPromptSubmit  etc. For instance Stop
hook runs after Claude finishes responding
and the UserPromptSubmit  hook runs when
user submits a prompt before Claude

processes it.
The first hook I created was to play an anime
notification sound when Claude stopped
responding. I was obviously inspired by
Cursor's notification sound.
One funny use case to run Claude for hours
might be running a "Do more" prompt when
Claude finishes current task via the Stop
hook.
"Do more" prompt via Stop hook to
keep Claude running. source

$PNCJOJOH)PPLT4LJMMTBOE
3FNJOEFST
I came across this post during my research for
this blog post. This person beautifully
combined the concepts and features we
discussed so far. They use hooks to remind
the model about the skill. If the
utility/requirement arises, there's a lot of
space for customization. You might not need
such heavy customization but can at least
take inspiration. (Speaking for myself lol)


Source: Claude Code is a beast -
tips from 6 months of usage
Anthropic recommends to keep skill.md under
500 lines so they divided it into separate files
and combined with hooks and reduced the
size of their CLAUDE.md.
Dividing instructions into skill files

to reduce CLAUDE.md size.
Source: same post
âœ± Coalescing...
$PODMVTJPO
Hopefully you learnt a bunch of things from
this super long post and will apply the
learnings not only in CC but other tools as
well. I feel a bit weird writing this but we are
going through some transformative times.
There are already moments when I almost feel
like a background agent and then other times
when I feel smart when the models couldn't
solve a particular bug.

sankalp
@dejavucoderÂ· Follow
claude and codex to me when i realise i am the 
background agent
7:26 AM Â· Dec 17, 2025

4.3K Reply Copy link
Read 31 replies
I no longer look forward to new releases
because they just keep happening anyways
(shoutout to OpenAI). Deepseek and Kimi K3
are in the queue.
I am expecting improvements in RL training,
long context effectiveness via maybe new
attention architectures, higher throughput
models, lesser hallucination models. There
might be a o1/o3 level reasoning breakthrough
or maybe something in continual learning in
2026. I look forward to these but at the same
time I find it scary because more significant
capability unlock will make the world

unpredictable haha.
Dario with mandate of heaven for
now
If you found this useful, try one new feature
from this post today. Happy building!
Thanks for reading. Please like/share/RT
the post if you liked it.

"DLOPXMFEHFNFOUT
Thanks to tokenbender, telt, debadree, matt,
pushkar for showing the courage to read the
final draft.
Thanks to Claude Opus 4.5 for editing and all
the Twitter people who have been quoted in
this post.
3FGFSFODFT
.Z1SFWJPVT1PTUT

My Experience With Claude Code After 2
Weeks of Adventures - July 2025
How Prompt Caching Works - Technical
deep dive
"OUISPQJD&OHJOFFSJOH#MPH
Building Effective Agents - Agent
architecture fundamentals
Effective Context Engineering for AI
Agents - Context management guide
Code Execution with MCP - MCP
patterns
$MBVEF$PEF%PDVNFOUBUJPO
Claude Code Changelog

Checkpointing
Agent Skills
3FTFBSDI5FDIOJDBM3FTPVSDFT
Context Rot - Research on context
degradation
Tool Calling Explained - Cursor's guide
What Actually Is Claude Code's Plan
Mode? - Armin Ronacher's analysis
Context Engineering for AI Agents:
Lessons from Building Manus
4ZTUFN1SPNQUT*OUFSOBMT
Claude Code System Prompts - Reverse
engineered prompts
System Prompt Extraction Video

$PNNVOJUZ3FTPVSDFT
awesome-claude-code - Commands,
hooks, skills collection
Claude Code is a beast - tips from 6
months of usage - Reddit post on
hooks/skills combo
5XJUUFS9%JTDVTTJPOT
Karpathy on keeping up
Addy Osmani's take
Boris (bcherny) on domain knowledge
Thariq's async agent use case
Prompt suggestions announcement
Peter's sub-agent shenanigans | Part 2
25

25
Powered by Bear Ê•â€¢á´¥â€¢Ê”